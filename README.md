# Playing Psychic: Using Thought Trees to Predict Reasoning Models Accuracy on Coding Tasks

We perform a systematic study of frontier reasoning models (e.g., DeepSeek-R1, QwQ) on real-world coding benchmarks. Our research demonstrates that the **structure** of a reasoning traceâ€”represented as a "Thought-Tree"â€”is a strong predictor of model correctness, often more so than the raw content alone.

## ðŸ“Œ Abstract

Recent advances in large language models (LLMs) have shown that test-time scaling can substantially improve performance on complex tasks, particularly in the coding domain. Under this paradigm, models use a larger token budget during inference to generate intermediate reasoning traces before producing a final answer. However, current evaluations primarily rely on competitive programming benchmarks, which may not capture the full range of reasoning abilities.

In this work, we:
1.  Devise a framework to **automatically generate coding tasks** of arbitrary difficulty and structure from existing benchmarks.
2.  Propose **Structured Thought-Trees** as a means to represent and analyze reasoning traces.
3.  Train a **lightweight classifier** on topological features extracted from these trees.

Our results show that this classifier can predict if a trace contains the correct answer with up to **89% accuracy** across three coding datasets.

## ðŸš€ Key Features

* **Task Generation Framework:** Programmatic generation of coding tasks to test specific reasoning capabilities.
* **Trace Segmentation:** NLP-based tools to break down raw LLM reasoning outputs into discrete thought segments.
* **Thought-Tree Construction:** Algorithms to reconstruct the hierarchical structure of reasoning (Continuation, Contrast, Rephrase).
* **Feature Extraction:** Extraction of graph-theoretic features (e.g., Branching Factor, Depth).
* **Correctness Classifier:** A lightweight model to predict task success based on trace structure.

## ðŸ“‚ Directory Structure

```text
.
â”œâ”€â”€ analysis/                   # Core logic for analyzing reasoning traces
â”‚   â”œâ”€â”€ codelingua/            
â”‚   â”œâ”€â”€ cruxeval/              
â”‚   â”œâ”€â”€ safim/                  
â”‚   â”œâ”€â”€ source_code/           
â”‚   â”‚   â”œâ”€â”€ clustering.py       # grouping similar reasoning patterns
â”‚   â”‚   â”œâ”€â”€ evaling.py          # Feature extraction (depth, branching, etc.)
â”‚   â”‚   â”œâ”€â”€ labeling.py         # Assigning structural labels to segments
â”‚   â”‚   â””â”€â”€ segmentation.py     # NLP-based reasoning trace segmentation
â”‚   â””â”€â”€ tree_analysis.ipynb     # Main notebook for training the classifier
â”œâ”€â”€ data_generation/            # Scripts to programmatically generate coding tasks
â”œâ”€â”€ llm-evaluation/             # Raw model outputs
â”‚   â”œâ”€â”€ codelingua/             
â”‚   â”œâ”€â”€ cruxeval/               
â”‚   â””â”€â”€ safim/                 
â””â”€â”€ README.md                   # Project documentation